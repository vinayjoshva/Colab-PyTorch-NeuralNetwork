{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQFJ0i3uFrNjsUInh2a8Rm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayjoshva/Colab-PyTorch-NeuralNetwork/blob/main/Practical_3_(16352584).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "njRXaWe0sjv7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "shuffle=True)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq5U7v8FuYnG",
        "outputId": "bd9e342e-2310-42aa-ebfd-dda4352fb5e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 41.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.32MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.16MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.11MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = CNN()"
      ],
      "metadata": {
        "id": "asYphZQHubLe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(3):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y2fiyOxu_we",
        "outputId": "e16daeb7-062a-4c9d-f086-b825a5366b54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1538939276233073\n",
            "Epoch 2, Loss: 0.04543734907082268\n",
            "Epoch 3, Loss: 0.032439657086626876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Disable gradient calculation for inference\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        outputs = model(images)  # Forward pass\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "        total += labels.size(0)  # Total number of labels\n",
        "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "# Print test accuracy\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao0NuFbfwORN",
        "outputId": "81316b1c-4876-4839-8bcc-2aef6622b98d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "index = random.randint(0, len(images) - 1)\n",
        "img = images[index].squeeze()\n",
        "true_label = labels[index].item()\n",
        "output = model(images[index].unsqueeze(0))\n",
        "predicted_label = torch.argmax(output).item()\n",
        "plt.imshow(img.numpy(), cmap='gray')\n",
        "plt.title(f\"Predicted: {predicted_label}, True: {true_label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "o66WNrpAyB7O",
        "outputId": "8bfbc7ef-5912-4972-dfbd-9400b9fd22c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "GEMINI_API_KEY = \"AIzaSyDmM6yLBN-1rWh0ylj80ZDCcNe1EAOCzxs\"\n",
        "GEMINI_ENDPOINT = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "predicted_label = 5  # Ensure this value exists\n",
        "prompt = f\"The CNN model predicted digit {predicted_label} for an image. Explain why it might have made this prediction.\"\n",
        "\n",
        "# Correct API request format\n",
        "data = {\n",
        "    \"contents\": [{\"parts\": [{\"text\": prompt}]}]  # Gemini API expects this format\n",
        "}\n",
        "\n",
        "# Send API request\n",
        "response = requests.post(GEMINI_ENDPOINT, headers=headers, json=data)\n",
        "\n",
        "# Print full API response for debugging\n",
        "response_json = response.json()\n",
        "print(\"Full API Response:\", response_json)\n",
        "\n",
        "# Extract and print explanation if available\n",
        "if \"candidates\" in response_json:\n",
        "    explanation = response_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    print(\"Gemini Explanation:\", explanation)\n",
        "else:\n",
        "    print(\"Error:\", response_json.get(\"error\", \"Unknown error\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REzyOPymyGYV",
        "outputId": "0bbef9ec-5a56-43e0-9d2c-0f31091ada24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full API Response: {'candidates': [{'content': {'parts': [{'text': \"**Possible Reasons for CNN Predicting Digit 5:**\\n\\n* **Similar Visual Features:** Digit 5 shares visual features with other digits, such as a curved shape and a gap on the bottom. If the image contains partial or occluded features that resemble these characteristics, the CNN may have mistaken it for a 5.\\n\\n* **Interference from Neighboring Digits:** If the image contains multiple digits, the CNN may have difficulty distinguishing between them. Features from neighboring digits may interfere with the recognition of the target digit, leading to a misclassification as 5.\\n\\n* **Rotating Variant:** Digit 5 has a rotational symmetry, which means it can appear in multiple orientations. If the image's rotation is not aligned with the CNN's training data, it may lead to incorrect recognition.\\n\\n* **Data Imbalance:** If the training data contains more images of digit 5 than other digits, the CNN may become biased towards predicting 5 in cases where the features are ambiguous.\\n\\n* **Model Overfitting:** Overfitting occurs when a CNN learns the training data too closely, resulting in poor generalization to new images. In this case, the CNN may have memorized specific patterns associated with the digit 5 and incorrectly applied them to the test image.\\n\\n* **Insufficient Training:** If the CNN has not been trained on sufficiently diverse or extensive data, it may lack the capacity to differentiate between similar digits like 5 and others.\\n\\n* **Noise or Artifacts:** Noise or artifacts in the image, such as blur or pixelation, can affect the CNN's ability to extract accurate features. If the noise resembles the shape or features of 5, it may lead to the misclassification.\\n\\n* **Incorrect Data Preprocessing:** Improper preprocessing steps, such as resizing or normalization, can alter the features of the image and make it difficult for the CNN to recognize the digit correctly.\"}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0, 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}], 'usageMetadata': {'promptTokenCount': 20, 'candidatesTokenCount': 382, 'totalTokenCount': 402}, 'modelVersion': 'gemini-pro'}\n",
            "Gemini Explanation: **Possible Reasons for CNN Predicting Digit 5:**\n",
            "\n",
            "* **Similar Visual Features:** Digit 5 shares visual features with other digits, such as a curved shape and a gap on the bottom. If the image contains partial or occluded features that resemble these characteristics, the CNN may have mistaken it for a 5.\n",
            "\n",
            "* **Interference from Neighboring Digits:** If the image contains multiple digits, the CNN may have difficulty distinguishing between them. Features from neighboring digits may interfere with the recognition of the target digit, leading to a misclassification as 5.\n",
            "\n",
            "* **Rotating Variant:** Digit 5 has a rotational symmetry, which means it can appear in multiple orientations. If the image's rotation is not aligned with the CNN's training data, it may lead to incorrect recognition.\n",
            "\n",
            "* **Data Imbalance:** If the training data contains more images of digit 5 than other digits, the CNN may become biased towards predicting 5 in cases where the features are ambiguous.\n",
            "\n",
            "* **Model Overfitting:** Overfitting occurs when a CNN learns the training data too closely, resulting in poor generalization to new images. In this case, the CNN may have memorized specific patterns associated with the digit 5 and incorrectly applied them to the test image.\n",
            "\n",
            "* **Insufficient Training:** If the CNN has not been trained on sufficiently diverse or extensive data, it may lack the capacity to differentiate between similar digits like 5 and others.\n",
            "\n",
            "* **Noise or Artifacts:** Noise or artifacts in the image, such as blur or pixelation, can affect the CNN's ability to extract accurate features. If the noise resembles the shape or features of 5, it may lead to the misclassification.\n",
            "\n",
            "* **Incorrect Data Preprocessing:** Improper preprocessing steps, such as resizing or normalization, can alter the features of the image and make it difficult for the CNN to recognize the digit correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFARCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model_cifar = CIFARCNN()"
      ],
      "metadata": {
        "id": "nnmgQDVWY4wz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB images\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class CIFARCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFARCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
        "model = CIFARCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train for 2 epochs\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(trainloader):.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move to GPU if available\n",
        "        outputs = model(images)  # Forward pass\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print final test accuracy\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyT2bgAKZTYM",
        "outputId": "ba0f9c8d-d4dd-44e0-b02f-1e56b8a88c9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 88.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch 1/2, Loss: 1.4771\n",
            "Epoch 2/2, Loss: 1.1468\n",
            "Test Accuracy: 61.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # COMPARISON BETWEEN MNSIT & CIFER-10\n",
        "\n",
        "# FEATURES\n",
        "# 1. Number of Classes - MNIST: 10, CIFAR-10: 10\n",
        "# 2. Image Size - MNIST: 28x28, CIFAR-10: 32x32\n",
        "# 3. Channel - MNIST: 1, CIFAR-10: 3\n",
        "# 4. Complexity Level - MNIST: Simple, CIFER-10: complex\n",
        "# 5. Input Shape - MNIST: (28, 28, 1), CIFAR-10: (32, 32, 3)\n",
        "# 6. Data Augmentation - MNIST: No, CIFAR-10: Yes\n",
        "# 7. Batch Size - MNIST: 32, CIFAR-10: 64\n",
        "# 8. Model Depth - MNIST: Shallow CNN, CIFER-10: Deep CNN\n",
        "# 9. Training Time - MNIST: Fast, CIFER-10: Slow\n",
        "# 10. Accuracy - MNIST: High, CIFER-10: Low\n",
        "\n",
        "# MNIST CNN: A Simple Yet Effective Model\n",
        "# The MNIST dataset consists of grayscale, handwritten digits (0-9) with an image size of 28×28 pixels and a single channel. Due to the simplicity of the dataset,\n",
        "# a relatively shallow Convolutional Neural Network (CNN) is sufficient to achieve high accuracy. The MNIST CNN architecture typically consists of two convolutional layers,\n",
        "# each followed by a max-pooling layer to reduce spatial dimensions while preserving important features. After feature extraction, the output is flattened and passed through\n",
        "# fully connected layers before reaching the final softmax layer for classification. Training a CNN on MNIST is computationally inexpensive, often running efficiently on a CPU.\n",
        "# The well-defined structure of digits and the limited number of classes allow\n",
        "# even a basic CNN to reach an accuracy of 90-99% within just a few training epochs. The relatively small number of parameters also minimizes overfitting, making this\n",
        "# a great beginner-friendly deep learning problem. Since the dataset has low intra-class variability (digits are similar in structure), the CNN primarily focuses on detecting edges,\n",
        "# curves, and strokes, which are simple features compared to real-world images.\n",
        "\n",
        "# CIFAR-10 CNN: A More Challenging Classification Task\n",
        "# The CIFAR-10 dataset, on the other hand, presents a more complex challenge, containing real-world RGB images from 10 different classes such as airplanes, cars, birds, cats, and more.\n",
        "# Unlike MNIST, these images have three color channels (RGB), are more detailed, and include significant variations in size, pose, background, and lighting conditions.\n",
        "# These factors make object classification significantly harder, requiring a deeper and more computationally demanding CNN.\n",
        "# The standard CIFAR-10 CNN architecture includes multiple convolutional layers with higher filter counts to detect complex features like textures, shapes, and colors.\n",
        "# However, due to the dataset's complexity, a simple two-layer CNN (like the one used for MNIST) often results in only ~60% accuracy, which is much lower than MNIST’s 90%+ accuracy.\n",
        "# To improve performance, deeper architectures such as VGG-16, ResNet, or EfficientNet are typically required. Additionally, data augmentation techniques like flipping, rotation, and brightness adjustments help improve model generalization.\n",
        "# Training on CIFAR-10 requires more computational resources, typically requiring a GPU for efficient processing.\n",
        "# Unlike MNIST, which relies on edge detection, CIFAR-10 requires the model to extract high-level semantic features like object shapes and textures, which makes it a far more difficult classification task.\n",
        "# Consequently, training a CNN on CIFAR-10 takes longer, requires more epochs, and benefits significantly from transfer learning with pre-trained models."
      ],
      "metadata": {
        "id": "Am1mixCseYM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}